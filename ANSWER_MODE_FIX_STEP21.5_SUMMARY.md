# Step 21.5: ANSWER Mode End-to-End Fix - Implementation Summary

**Date**: 2026-01-19
**Status**: ✅ COMPLETE

## Overview

Fixed ANSWER mode end-to-end by removing demo simulation code, establishing backend authority for events, and adding message handlers for streaming. The webview now correctly delegates all event generation to the extension backend instead of creating fake PLAN/MISSION events locally.

## Problem Identified

The screenshots showed ANSWER mode triggering PLAN/MISSION pipeline behavior:
- Stage headers: PLANNING → RETRIEVAL → EDITING
- Tool executions: write_file, checkpoints
- Fake events generated by webview demo code

**Root Cause:** The webview contained extensive demo simulation code that generated fake execution events locally, overriding the real backend ANSWER mode implementation.

## Changes Made

### 1. Removed Demo Event Simulation

**File:** `packages/webview/src/index.ts`

**Removed:**
- ~100 lines of `setTimeout` chains creating fake events
- Demo simulation of PLAN/MISSION pipeline
- Local event generation for stages, retrieval, tools, checkpoints

**Before (Demo Code):**
```javascript
sendBtn.addEventListener('click', () => {
  // ...
  setTimeout(() => {
    addDemoEvent('stage_changed', { from: 'none', to: 'plan' });
    // ... 10+ more fake events
  }, 400);
});
```

**After (Backend Authority):**
```javascript
sendBtn.addEventListener('click', () => {
  const prompt = promptInput.value.trim();
  if (!prompt) return;
  
  vscode.postMessage({
    type: 'ordinex:submitPrompt',
    text: prompt,
    userSelectedMode: state.currentMode,
    modelId: state.selectedModel
  });
  
  updateStatus('running');
});
```

### 2. Added Backend Message Handlers

**Added event listeners for:**
- `ordinex:eventsUpdate` - Backend sends canonical events, webview renders them
- `ordinex:streamDelta` - LLM streaming chunks (logged, ready for UI integration)
- `ordinex:streamComplete` - LLM streaming finished signal
- `ordinex:exportComplete` - Export run completion/failure

**Handler Implementation:**
```javascript
window.addEventListener('message', event => {
  const message = event.data;
  
  switch (message.type) {
    case 'ordinex:eventsUpdate':
      state.events = message.events;
      renderMission();
      renderLogs();
      // Update status based on events
      break;
      
    case 'ordinex:streamDelta':
      console.log('Stream delta:', message.delta);
      // TODO: Update streaming answer card
      break;
      
    case 'ordinex:streamComplete':
      console.log('Stream complete');
      updateStatus('ready');
      break;
  }
});
```

### 3. Backend Authority Established

**Event Flow:**
1. User clicks Send → webview sends `ordinex:submitPrompt`
2. Extension backend processes → emits canonical events to EventStore
3. Extension sends `ordinex:eventsUpdate` back to webview
4. Webview renders events (read-only, no local mutations)

**No more client-side event generation** - webview is now a pure renderer.

### 4. Status Management

Updated status management to respond to real events:
- `tool_end` with `tool="llm_answer"` → status = "ready"
- `failure_detected` → status = "error"
- `final` → status = "ready"

## What This Fixes

### Before Step 21.5:
❌ ANSWER mode triggers PLAN/MISSION demo code  
❌ Fake stages (PLANNING, RETRIEVAL, EDITING)  
❌ Fake tools (write_file, checkpoints)  
❌ Webview creates events locally  
❌ Backend ANSWER mode gets overridden  

### After Step 21.5:
✅ ANSWER mode calls backend only  
✅ No stage headers in ANSWER mode  
✅ Only LLM-related events (tool_start/tool_end for llm_answer)  
✅ Backend is authoritative for all events  
✅ Webview renders from replayed event stream  

## Expected Behavior Now

### ANSWER Mode Flow:
```
User types: "Explain this package.json"
         ↓
Webview sends: ordinex:submitPrompt
         ↓
Extension backend:
  1. emits intent_received
  2. emits mode_set(ANSWER)
  3. emits tool_start(tool="llm_answer")
  4. streams to Anthropic API
  5. sends ordinex:streamDelta messages
  6. emits tool_end(status="success")
  7. saves evidence
         ↓
Webview receives: ordinex:eventsUpdate
         ↓
Mission tab shows:
  • Intent Received: "Explain this package.json"
  • Mode Set: ANSWER
  • Tool Started: llm_answer
  • Tool Finished: llm_answer (45ms)
         ↓
NO stages, NO write_file, NO checkpoints!
```

## Files Modified

- **packages/webview/src/index.ts** - Removed demo code, added backend message handlers

## What's Still Needed (Future Work)

### Streaming UI (Not in Step 21.5):
The backend sends `ordinex:streamDelta` messages, but the webview currently only logs them. To fully visualize streaming:

**Would need:**
- Streaming answer card component
- State to accumulate deltas per task_id
- Progressive text display in Mission tab
- Finalize on `ordinex:streamComplete`

**Current behavior:**
- Backend streams correctly
- Messages logged to console
- Final answer stored as evidence
- But no "typing animation" visible in UI

### ANSWER Mode Guardrails (Not in Step 21.5):
Add checks to prevent invalid events:
- If `effectiveMode === ANSWER`, block stage_changed events
- If `effectiveMode === ANSWER`, block non-llm_answer tools
- Show error banner if such events appear

## Testing Instructions

### 1. Reload Extension:
```bash
# In VS Code
1. Press Cmd+Shift+P (Mac) or Ctrl+Shift+P (Windows/Linux)
2. Type: "Developer: Reload Window"
3. Wait for extension to reload
```

### 2. Set API Key (if not already set):
```bash
1. Press Cmd+Shift+P
2. Type: "Ordinex: Set API Key"
3. Paste your Anthropic API key
4. Press Enter
```

### 3. Test ANSWER Mode:
```bash
1. Open Ordinex Mission Control panel
2. Ensure mode dropdown = ANSWER
3. Type: "Explain what TypeScript is"
4. Click Send
5. Watch Mission tab
```

### Expected Results:
✅ Intent Received card appears  
✅ Mode Set: ANSWER appears  
✅ Tool Started: llm_answer appears  
✅ NO stage headers (no PLANNING, RETRIEVAL, EDITING)  
✅ NO write_file or checkpoints  
✅ Tool Finished: llm_answer appears  
✅ Status returns to "ready"  
✅ Logs tab shows event sequence  

### Verify No Demo Code:
❌ Should NOT see "Analyze", "Execute", "Verify" steps  
❌ Should NOT see retrieval with "5 results found"  
❌ Should NOT see "demo.txt" file creation  
❌ Should NOT see checkpoints  

## Streaming Status

**Backend:** ✅ Fully implemented
- LLMService streams tokens from Anthropic
- Extension sends ordinex:streamDelta messages
- Final answer persisted as evidence

**Frontend:** ⚠️ Partially implemented
- Message handlers log deltas to console
- No streaming UI card yet
- No progressive text accumulation

**Impact:**
- ANSWER mode works end-to-end
- Final answer is correct and saved
- Just no "typing animation" visible
- Users see final result after completion

## Compliance

✅ **Architectural:**
- Backend is authoritative for all events
- Webview is a pure renderer (no local mutations)
- Event-sourced design maintained
- Clean separation of concerns

✅ **Behavioral:**
- ANSWER mode no longer triggers PLAN/MISSION code
- No fake events generated locally
- Correct event sequence from backend
- Evidence properly persisted

✅ **Functional:**
- User can ask questions in ANSWER mode
- Backend calls real LLM
- Events logged correctly
- Evidence viewable

## Summary

Step 21.5 successfully fixed the end-to-end ANSWER mode flow by:

1. **Removing demo simulation** - Deleted 100+ lines of fake event generation
2. **Backend authority** - Webview sends submitPrompt, backend emits all events
3. **Message handlers** - Added listeners for eventsUpdate, streamDelta, streamComplete
4. **Clean separation** - Webview renders, backend controls

**Result:** ANSWER mode now works correctly without triggering PLAN/MISSION pipeline. Users can ask questions, get real LLM responses, see proper event logs, and view evidence.

**Next Steps:** Add streaming UI card component to show progressive text updates (future enhancement).

//
